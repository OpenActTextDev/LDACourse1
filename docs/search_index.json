[
["index.html", "Loss Data Analytics: Short Course Preface Why Loss Data Analytics? Resources How to Navigate this Tutorial A word from our Sponsor: Astin + Education = Astin Academy Contributors", " Loss Data Analytics: Short Course A short course authored by the Actuarial Community Preface Date: 26 March 2021 Why Loss Data Analytics? The intent is that this type of short course will eventually permeate throughout the actuarial curriculum. Given the dramatic changes in the way that actuaries treat data, analysis of loss data seems like a natural place to start. The idea behind the name loss data analytics is to integrate classical loss data models from applied probability with modern analytic tools. In particular, we recognize that big data (including social media and usage based insurance) are here to stay and that high speed computation is readily available. Resources This tutorial is based on the first five chapters of the open and freely available book Loss Data Analytics. Sample code is available for the book, although not needed for this tutorial. As described in how to navigate this tutorial below, you will not need to download any data for this course. However, if you want to have the data to follow up, we generally make it available through buttons such as: View the Property Fund Data as a .csv file. Then, do a “Save Page as” if you wish to download. Alternatively, go directly to the Github page to access the data. For advanced data applications in insurance, you may be interested in the series, Predictive Modeling Applications in Actuarial Science. How to Navigate this Tutorial This online tutorial is designed to guide you through the foundations of modeling loss data. The anticipated completion time is approximately six hours. The tutorial assumes that you are familiar with the foundations in the statistical software R, such as Datacamp’s Introduction to R. Role of Video Introductions. There are five chapters in this tutorial that summarize the foundations. Each chapter is subdivided into several sections. At the beginning of each section is a short video, typically 4-8 minutes, that summarizes the section key learning outcomes. Following the video, you can see more details about the underlying R code for the analysis presented in the video. Role of Exercises. Following each video introduction, there are one or two exercises that allow you to practice skills to make sure that you fully grasp the learning outcomes. The exercises are implemented using an online learning platform provided by Datacamp so that you need not install R. Feedback is programmed into the exercises so that you will learn a lot by making mistakes! You will be pacing yourself, so always feel free to reveal the answers by hitting the Solution tab. Remember, going through quickly is not equivalent to learning deeply. Use this tool to enhance your understanding of one of the foundations of data science. Video: Overview of Tutorial Navigation A word from our Sponsor: Astin + Education = Astin Academy In this video, you learn about: the role of the ASTIN section, the non-life section of the International Actuarial Association their recent interest in education their development of the recently formed Astin Academy. “The mission of the ASTIN Academy is to provide high quality free educational materials in non-life insurance to actuarial students across the world. The Academy is committed to open course development under Creative Commons licence and seeks innovation in teaching methodologies and application of technology. Let’s build the future of actuarial education together.” Video: Overview of the Astin Academy Contributors The project goal is to have the actuarial community author our textbooks in a collaborative fashion. The following contributors have taken a leadership role in developing this short course for \\(Loss\\) \\(Data\\) \\(Analytics\\). Yvonne Chueh is a professor at Central Washington University. She received PhD from the University of Connecticut, bachelor degree in pure math from National Taiwan University. She has taught actuarial courses at UCONN and UW-Eau Claire. She is an associate (since 1994) and council member/chair (2013-2017/2017) of the Society of Actuaries (SOA) Education &amp; Research Section. Her research works were published by the American Academy of Actuaries, the Actuarial Foundation, SOA North American Actuarial Journal, Electrical Engineering Series of Springer, International Joint Conference on Neural Networks, IEEE Symposium Series on Computational Intelligence (SSCI), and International Journal of Computers, Communications &amp; Control. Miyuki Ebisaki is a Senior Research Officer of SOMPO Research Institute Inc. in Japan. Her research work is in the area of insurance, health care, social economy and new technologies. She has been working in domestic and foreign insurers in Japan and in the UK. She has various experiences both in non-life and life insurance; including product development, chief actuary, M&amp;A and risk management. In May 2020, she was elected as a member of the ASTIN board. Rob Erhardt, Wake Forest University Edward W. (Jed) Frees is an emeritus professor, formerly the Hickman-Larson Chair of Actuarial Science at the University of Wisconsin-Madison and currently affiliated with Australian National University. He is a Fellow of both the Society of Actuaries and the American Statistical Association. He has published extensively (a four-time winner of the Halmstad Prize for best paper published in the actuarial literature) and has written three books. He also is a co-editor of the two-volume series Predictive Modeling Applications in Actuarial Science published by Cambridge University Press. Brian Hartman, Brigham Young University Tim Higgins, Australian National University Fei Huang is a Senior Lecturer in the School of Risk and Actuarial Studies, University of New South Wales (UNSW) Sydney. Before joining UNSW, she worked at the Australian National University (ANU) as Senior Lecturer (2019-2020) and Lecturer (2015-2018). Fei’s main research interest lies in predictive modelling and data analytics for insurance applications. She is an active researcher and publishes papers frequently in top-tier actuarial journals. Fei is also a dedicated educator. Her educational excellence has been recognized by winning the ANU Vice Chancellor’s Award for Teaching Excellence in the Early Career Category (2018) and ANU College of Business and Economics Award for Teaching Excellence in the Early Career Category (2017). Himchan Jeong is currently an Assistant Professor at Simon Fraser University. He is a Fellow of the Society of Actuaries (SOA) and holds a Ph.D. from the University of Connecticut. He has been actively involved in teaching and conducting research in actuarial science for several years. In recognition for his academic achievements and excellence, he has been awarded the James C. Hickman Scholarship from SOA recently in 2018-2020. His current research interest is predictive modeling for ratemaking and reserving of property and casualty insurance. Paul H. Johnson, Jr. is a lecturer in the Risk and Insurance Department of the Wisconsin School of Business at the University of Wisconsin-Madison. He has an MS in Actuarial Science and a PhD in Business (Actuarial Science), both from UW-Madison. Paul teaches undergraduate courses in life contingencies, loss models, regression, and analytics. Prior to teaching at UW-Madison, Paul received various honors and awards for teaching excellence in the Department of Mathematics at the University of Illinois at Urbana-Champaign; most notably, the N. Tenney Peck Teaching Award in Mathematics. He also has published articles in the North American Actuarial Journal and the Risk Management and Insurance Review. Joseph H.T. Kim, Ph.D., FSA, CERA, is Associate Professor of Applied Statistics at Yonsei University, Seoul, Korea. He holds a Ph.D. degree in Actuarial Science from the University of Waterloo, at which he taught as Assistant Professor. He also worked in the life insurance industry. He has published papers in Insurance Mathematics and Economics, Journal of Risk and Insurance, Journal of Banking and Finance, ASTIN Bulletin, and North American Actuarial Journal, among others. Rajesh Sahasrabuddhe, Oliver Wyman Peng Shi is an associate professor in the Risk and Insurance Department at the Wisconsin School of Business. He is also the Charles &amp; Laura Albright Professor in Business and Finance. Professor Shi is an Associate of the Casualty Actuarial Society (ACAS) and a Fellow of the Society of Actuaries (FSA). He received a Ph.D. in actuarial science from the University of Wisconsin-Madison. His research interests are problems at the intersection of insurance and statistics. He has won several research awards, including the Charles A. Hachemeister Prize, the Ronald Bornhuetter Loss Reserve Prize, and the American Risk and Insurance Association Prize. Nariankadu D. Shyamalkumar (Shyamal) is an associate professor in the Department of Statistics and Actuarial Science at The University of Iowa. He is an Associate of the Society of Actuaries, and has volunteered in various elected and non-elected roles within the SoA. Having a broad theoretical interest as well as interest in computing, he has published in prominent actuarial, computer science, probability theory, and statistical journals. Moreover, he has worked in the financial industry, and since then served as an independent consultant to the insurance industry. He has experience educating actuaries in both Mexico and the US, serving in the roles of directing an undergraduate program, and as a graduate adviser for both masters and doctoral students. Zhiyu (Frank) Quan is an Assistant Professor at the Department of Mathematics of the University of Illinois at Urbana-Champaign. He holds a Ph.D. in Actuarial Science from the University of Connecticut. Before joining Illinois, he worked for a cutting-edge Insurtech company as a R &amp; D data scientist developing data-driven solutions for major insurance companies. He has a broad spectrum of research interests in data science applications in insurance such as tree-based models, natural language processing, deep learning, and applies his actuarial expertise to build predictive models for claim research, rate making, etc. His research projects are driven by real-life data and are inspired from collaborations with Insurtech and insurance companies. Besides, he is a faculty advisor for the Illinois Risk Lab, which facilitates research activities that integrate academic training with practical problem-solving in real business settings. He recently has received the Arnold O. Beckman Research Award. Michelle Xia is an Associate Professor in in the Department of Statistics and Actuarial Science at Northern Illinois University. Michelle earned her Ph.D. in statistics from the University of British Columbia. Besides research, teaching and consulting at NIU, Michelle has over seven years of professional experience as an actuary, a predictive modeler and a statistician in the insurance and medical areas. Michelle’s research is motivated by real-life problems, with current interests including predictive analytics, misrepresentation modeling, dependence modeling, insurance ratemaking and loss reserving. "],
["introduction-to-loss-data-analytics.html", "Chapter 1 Introduction to Loss Data Analytics 1.1 Relevance of Analytics to Insurance Activities 1.2 Insurance Company Operations 1.3 Case Study: Property Fund Introduction 1.4 Property Fund Claim Severity 1.5 Property Fund Rating Variables", " Chapter 1 Introduction to Loss Data Analytics Chapter description This course introduces readers to methods of analyzing insurance data. Section 1.1 begins with a discussion of why the use of data is important in the insurance industry. Section 1.2 gives a general overview of the purposes of analyzing insurance data which is reinforced in the Section 1.3 case study. Naturally, there is a huge gap between the broad goals summarized in the overview and a case study application; this gap is covered through the methods and techniques of data analysis covered in the rest of the text. Although not needed to go through the tutorials, some users may wish to download the overheads used in the videos. Download Chapter One overheads as a .pdf file. 1.1 Relevance of Analytics to Insurance Activities In this section, you learn how to: Summarize the importance of insurance to consumers and the economy Describe analytics Identify data generating events associated with the timeline of a typical insurance contract Video: Insurance and Analytics Overheads: Insurance and Analytics (Click Tab to View) A. Relevance of Insurance B. Analytics and Loss Data C. What is Analytics? D. Insurance Processes E. Relevance of Analytics Hide Hide Hide Hide Hide Show Quiz Solution 1.2 Insurance Company Operations In this section, you learn how to: Describe five major operational areas of insurance companies. Identify the role of data and analytics opportunities within the pricing area. Video: Insurance Company Operations Overheads: Insurance Company Operations (Click Tab to View) A. Insurance Company Operations I B. Insurance Company Operations II C. Operations – Initiating Insurance D. Big Data Hide Hide Hide Hide Show Quiz Solution 1.3 Case Study: Property Fund Introduction In this section, we use the Wisconsin Property Fund as a case study. You learn how to: Describe how data generating events can produce data of interest to insurance analysts. Produce relevant summary statistics for each variable. Describe how these summary statistics can be used to develop the cost of insurance. Video: Introducing the Wisconsin Property Fund Overheads: Introducing the Wisconsin Property Fund (Click Tab to View) A. Wisconsin Property Fund B. LGPIF Policyholder A C. LGPIF Policyholder B D. Property Fund E. Claims Frequency - R Code F. Claims Frequency (2010) Hide Hide Hide Hide Hide Hide 1.3.1 Exercise. Claim Frequency Assignment Text The Wisconsin Property Fund data has already been read into a data frame called Insample. These data consist of claim experience for fund members over the years 2006-2010, inclusive. It includes the frequency of claims Freq as well as the claim year Year. The video explored the distribution of the claims frequency for year 2010; in this assignment, we replicate this analysis and conduct a similar investigation for year 2009. Instructions. For each year: Use the function subset() to create a smaller data set based on a single year. Define the frequency as a global variable. Use the function length() to determine the number of observations in a vector. Use the function mean() to calculate the average. Use the function table() to tabulate the frequency distribution. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6Ikluc2FtcGxlIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL09wZW5BY3RUZXh0RGV2L0xEQUNvdXJzZTEvbWFpbi9EYXRhL0luc2FtcGxlLmNzdlwiLCBoZWFkZXI9VCxcbiAgICAgICAgICAgICAgICAgICAgICBuYS5zdHJpbmdzPWMoXCIuXCIpLCBzdHJpbmdzQXNGYWN0b3JzPUZBTFNFKSIsInNhbXBsZSI6IiMgIEFuYWx5c2lzIGZvciBZZWFyIDIwMTBcbkluc2FtcGxlMjAxMCA8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMTApXG5GcmVxMjAxMCA8LSBJbnNhbXBsZTIwMTAkRnJlcVxubGVuZ3RoKEZyZXEyMDEwKVxubWVhbihGcmVxMjAxMClcbnRhYmxlKEZyZXEyMDEwKVxuXG4jICBBbmFseXNpcyBmb3IgWWVhciAyMDA5XG5JbnNhbXBsZTIwMDkgPC0gX19fXG5GcmVxMjAwOSA8LSBfX19cbmxlbmd0aChfX18pXG5tZWFuKF9fXylcbnRhYmxlKF9fXykiLCJzb2x1dGlvbiI6IiMgIEFuYWx5c2lzIGZvciBZZWFyIDIwMTBcbkluc2FtcGxlMjAxMCA8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMTApXG5GcmVxMjAxMCA8LSBJbnNhbXBsZTIwMTAkRnJlcVxubGVuZ3RoKEZyZXEyMDEwKVxubWVhbihGcmVxMjAxMClcbnRhYmxlKEZyZXEyMDEwKVxuXG4jICBBbmFseXNpcyBmb3IgWWVhciAyMDA5XG5JbnNhbXBsZTIwMDkgPC0gc3Vic2V0KEluc2FtcGxlLCBZZWFyPT0yMDA5KVxuRnJlcTIwMDkgPC0gSW5zYW1wbGUyMDA5JEZyZXFcbmxlbmd0aChGcmVxMjAwOSlcbm1lYW4oRnJlcTIwMDkpXG50YWJsZShGcmVxMjAwOSkiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkdldHRpbmcgc3RhcnRlZCBpcyBhbHdheXMgdGhlIGhhcmRlc3QgdGhpbmcgdG8gZG8uIEV4Y2VsbGVudCBzdGFydCFcIikiLCJoaW50IjoiVGFrZSBzb21lIHRpbWUgdG8gZXhwbG9yZSB0aGUgb25saW5lIDxjb2RlPlI8L2NvZGU+IGRvY3VtZW50YXRpb24uPC9wPlxuXG48cD5Ob3RlIHRoZSBkb3VibGUgZXF1YWwgc2lnbnMgPGNvZGU+PT08L2NvZGU+IG5lZWRlZCBmb3IgdGhlIHRoZSA8Y29kZT5zdWJzZXQoKTwvY29kZT4gZnVuY3Rpb24uIn0= 1.4 Property Fund Claim Severity Video: Claim Severity for the Wisconsin Property Fund Overheads: Claim Severity for the Wisconsin Property Fund (Click Tab to View) A. Severity Distribution (2010) B. Claims Severity - R Code C. Claim Outcomes and Coverage by Year D. Analysis by Year - R Code E. Claim Frequency and Severity, Deductibles, and Coverage F. Claim Frequency and Severity, Deductibles, and Coverage - R Code G. Cost of Insurance Hide Hide Hide Hide Hide Hide Hide 1.4.1 Exercise. Exploring Claim Severities Assignment Text The Wisconsin Property Fund data covering years 2006-2010 has already been read into a data frame called Insample. These data includes the average claim severity yAvg. The video explored the distribution of the claims severity for year 2010; in this assignment, we conduct a similar investigation for year 2009. Instructions Use the function subset() twice, once to create a smaller data set based on year 2009 experience and again to restrict consideration to fund members with positive claims. From this subset, define the average claim severity as a global variable. Use the function length() to determine the number of members with positive claims. Use the function summary() to calculate several summary statistics. Use the function hist() to produce histograms that visualize the claim severity distribution. Do this once on the original scale and again on the logarithmic scale. Use the mfrow switch in the function par() to set the graphical parameters so that the graphs can be viewed side-by-side to promote comparisons. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6Ikluc2FtcGxlIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL09wZW5BY3RUZXh0RGV2L0xEQUNvdXJzZTEvbWFpbi9EYXRhL0luc2FtcGxlLmNzdlwiLCBoZWFkZXI9VCwgbmEuc3RyaW5ncz1jKFwiLlwiKSwgc3RyaW5nc0FzRmFjdG9ycz1GQUxTRSkiLCJzYW1wbGUiOiJJbnNhbXBsZTIwMDkgPC0gX19fXG5JbnNhbXBsZVBvczIwMDkgPC0gc3Vic2V0KEluc2FtcGxlMjAwOSwgeUF2Zz4wKVxuX19fIDwtIEluc2FtcGxlUG9zMjAwOSR5QXZnXG5cbiMgVGFibGVcbnN1bW1hcnkoX19fKVxuX19fKEF2Z0NsYWltKVxuXG4jIEZpZ3VyZXNcbnBhcihtZnJvdz1jKDEsIF9fXykpXG5oaXN0KEF2Z0NsYWltLCBtYWluPVwiXCIsIHhsYWI9XCJBdmVyYWdlIENsYWltc1wiKVxuaGlzdChfX18sIG1haW49XCJcIiwgeGxhYj1cIkxvZ2FyaXRobWljIEF2ZXJhZ2UgQ2xhaW1zXCIpIiwic29sdXRpb24iOiJJbnNhbXBsZTIwMDk8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMDkpXG5JbnNhbXBsZVBvczIwMDk8LSBzdWJzZXQoSW5zYW1wbGUyMDA5LCB5QXZnPjApXG5BdmdDbGFpbSA8LSBJbnNhbXBsZVBvczIwMDkkeUF2Z1xuXG4jIFRhYmxlXG5zdW1tYXJ5KEF2Z0NsYWltKVxubGVuZ3RoKEF2Z0NsYWltKVxuXG4jIEZpZ3VyZXNcbnBhcihtZnJvdz1jKDEsIDIpKVxuaGlzdChBdmdDbGFpbSwgbWFpbj1cIlwiLCB4bGFiPVwiQXZlcmFnZSBDbGFpbXNcIilcbmhpc3QobG9nKEF2Z0NsYWltKSwgbWFpbj1cIlwiLCB4bGFiPVwiTG9nYXJpdGhtaWMgQXZlcmFnZSBDbGFpbXNcIikiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkV4Y2VsbGVudCEgVGhpcyBleGVyY2lzZSBmZWF0dXJlcyBkYXRhIG1hbmlwdWxhdGlvbiAodGhyb3VnaCBzdWJzZXR0aW5nKSwgY2FsY3VsYXRlcyBzdW1tYXJ5IHN0YXRpc3RpY3MsIGFuZCBwb3J0cmF5cyBkYXRhIGdyYXBoaWNhbGx5LiBZb3UgaGF2ZSBkb25lIGl0IGFsbCFcIikiLCJoaW50IjoiVGhlIDxjb2RlPlI8L2NvZGU+IGZ1bmN0aW9uIDxhIGhyZWY9XCJodHRwczovL3d3dy5yZG9jdW1lbnRhdGlvbi5vcmcvcGFja2FnZXMvYmFzZS92ZXJzaW9ucy8zLjYuMi90b3BpY3MvbG9nXCI+bG9nKCk8L2E+IGRlZmF1bHRzIHRvIG5hdHVyYWwgbG9nYXJpdGhtcy4gSXQgY2FuIGFsc28gYmUgdXNlZCB0byBwcm9kdWNlIGNvbW1vbiAoYmFzZSAxMCkgYXMgd2VsbCBhcyBvdGhlciBiYXNlcy4ifQ== 1.4.2 Exercise. Skewness of the Claim Severity Distribution Assignment Text Many distributions used in insurance analytics are skewed. In this assignment, we demonstrate how to detect the skewness and show that skewness is a feature of an entire data set, not the result of a handful of outlying observations. Specifically, we consider the average claim for year 2010 of the Wisconsin Property Fund data that has been made available in the variable AvgClaim; this variable is only for fund members with positive claims. The video remarked on the skewness of this variable and used a logarithmic transformation to facilitate the analysis. In this assignment, we show how an alternative strategy, omitting unusually large observations, does not address the skewed nature of the distribution. Instructions Use the functions mean() and median() to compute both the mean and median of AvgClaim. You will see that the mean is much larger than the median, a classic diagnostic approach for establishing the right skewness of a distribution. Use the function order() to order the data in terms of decreasing size. Use the function head() to examine the 10 largest observations. Create 6 histograms, graphically presenting them in panels that have 2 rows and 3 columns. In the first histogram, omit the 9 largest observations. In the second, omit the 6 largest. Continuing this pattern, omit 5 in the third histogram, omit 3 in the fourth, omit 1 in the fifth, and do not admit any in the sixth. A careful inspection of this panel of histograms reveals that the distribution remains skewed, despite repeatedly removing large outlying observations. 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 1.4.3 Exercise. Claim Severity by Year Assignment Text Much of insurance analytics is about making predictions of future behavior. To understand distributions in future years, we look at trends in distributions from the current and preceding years. Some simple trends that are helpful not only for understanding but also communicating results to others involve looking at mean, or average, behavior. In this assignment, we examine 2006-2010 of the Wisconsin Property Fund that has been previously load into the data frame Insample. As in the video, by Year we calculate mean values of claims frequency (Freq), average claims severity (yAvg) and coverage (BCcov), as well as the number of fund members. Instructions Use the function ddply() to produce a data frame with the requisite summary statistics. To document results, provide intuitive names of these summary statistics. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6Ikluc2FtcGxlIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL09wZW5BY3RUZXh0RGV2L0xEQUNvdXJzZTEvbWFpbi9EYXRhL0luc2FtcGxlLmNzdlwiLCBoZWFkZXI9VCxcbiAgICAgICAgICAgICAgICAgICAgICBuYS5zdHJpbmdzPWMoXCIuXCIpLCBzdHJpbmdzQXNGYWN0b3JzPUZBTFNFKSIsInNhbXBsZSI6ImxpYnJhcnkoX19fKVxuVGFibGUxSW4gPC0gZGRwbHkoSW5zYW1wbGUsIGMoXCJZZWFyXCIpLCBzdW1tYXJpemUsXG4gICAgICAgICAgICAgICBBdmdGcmVxICAgID0gbWVhbihGcmVxKSwgQXZnU2V2ID0gX19fKHlBdmcpLFxuICAgICAgICAgICAgICAgQXZnQ292ICAgPSBfX18oQkNjb3YpLCBOdW1Qb2wgPSBfX18oRnJlcSkgKVxubmFtZXMoVGFibGUxSW4pIDwtIGMoXCJZZWFyXCIsIFwiQXZlcmFnZSBGcmVxdWVuY3lcIixcbiAgICAgICAgICAgICAgICAgICAgIFwiQXZlcmFnZSBTZXZlcml0eVwiLCBcIkF2ZXJhZ2UgQ292ZXJhZ2VcIixcbiAgICAgICAgICAgICAgICAgICAgIFwiTnVtYmVyIG9mIFBvbGljeWhvbGRlcnNcIilcblRhYmxlMUluIiwic29sdXRpb24iOiJsaWJyYXJ5KHBseXIpXG5UYWJsZTFJbiA8LSBkZHBseShJbnNhbXBsZSwgYyhcIlllYXJcIiksIHN1bW1hcml6ZSxcbiAgICAgICAgICAgICAgIEF2Z0ZyZXEgICAgPSBtZWFuKEZyZXEpLCBBdmdTZXYgPSBtZWFuKHlBdmcpLFxuICAgICAgICAgICAgICAgQXZnQ292ICAgPSBtZWFuKEJDY292KSwgTnVtUG9sID0gbGVuZ3RoKEZyZXEpIClcbm5hbWVzKFRhYmxlMUluKSA8LSBjKFwiWWVhclwiLCBcIkF2ZXJhZ2UgRnJlcXVlbmN5XCIsXG4gICAgICAgICAgICAgICAgICAgICBcIkF2ZXJhZ2UgU2V2ZXJpdHlcIiwgXCJBdmVyYWdlIENvdmVyYWdlXCIsXG4gICAgICAgICAgICAgICAgICAgICBcIk51bWJlciBvZiBQb2xpY3lob2xkZXJzXCIpXG5UYWJsZTFJbiIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50IVwiKSIsImhpbnQiOiJUbyBjb21wdXRlIHN0YW5kYXJkIGRldmlhdGlvbnMsIHlvdSBjYW4gdXNlIHRoZSBmdW5jdGlvbiA8YSBocmVmPVwiaHR0cHM6Ly93d3cucmRvY3VtZW50YXRpb24ub3JnL3BhY2thZ2VzL3N0YXRzL3ZlcnNpb25zLzMuNi4yL3RvcGljcy9zZFwiPnNkKCk8L2E+LiJ9 1.5 Property Fund Rating Variables Video: Rating Variables for the Wisconsin Property Fund Overheads: Rating Variables for the Wisconsin Property Fund (Click Tab to View) A. Description of Rating Variables B. Claims by Entity Type, Fire Class, and No Claim Credit C. Claim Frequency and Severity, Deductibles, and Coverage - R Code D. Claims by Entity Type and Alarm Credit Category E. Initiating Insurance Hide Hide Hide Hide Hide 1.5.1 Exercise. Claim by Entity Type, Fire Class, and No Claim Credit Assignment Text Analytics of insurance data has much in common with analytics in other areas but insurance also has its own peculiarities. One of example of this is how one only examines summary statistics for positive claims. (Mathematically, the claim severity distribution is defined conditional on the occurrence of a claim). As a consequence, we often need to write specialized routines for insurance applications. Your supervisor finds useful the analysis completed of claims outcomes by entity type, fire class, and no claim code. To make this analysis more impactful, the supervisor wants you to modify the code to include a basic measure of uncertainty, the standard deviation. Instructions Modify the function created for this specific analysis, ByVarSumm, to include the standard deviation statistic. Note that, like the mean, we only calculate the standard deviation on the sub-sample of positive claims. Include documentation of this standard deviation statistic in the table. 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 Show Quiz Solution "],
["frequency-modeling.html", "Chapter 2 Frequency Modeling", " Chapter 2 Frequency Modeling Placeholder "],
["modeling-loss-severity.html", "Chapter 3 Modeling Loss Severity", " Chapter 3 Modeling Loss Severity Placeholder "],
["model-selection-and-estimation.html", "Chapter 4 Model Selection and Estimation", " Chapter 4 Model Selection and Estimation Placeholder "],
["aggregate-loss-models.html", "Chapter 5 Aggregate Loss Models 5.1 Introduction 5.2 Individual Risk Model 5.3 Collective Risk Model - Part I 5.4 Collective Risk Model - Part II 5.5 Tweedie Distribution 5.6 Effects of Coverage Modifications", " Chapter 5 Aggregate Loss Models Chapter description This chapter introduces probability models for describing the aggregate (total) claims that arise from a portfolio of insurance contracts. We present two standard modeling approaches, the individual risk model and the collective risk model. Further, we discuss strategies for computing the distribution of the aggregate claims, including exact methods for special cases, recursion, and simulation. Finally, we examine the effects of individual policy modifications such as deductibles, coinsurance, and inflation, on the frequency and severity distributions, and thus on the aggregate loss distribution. Although not needed to go through the tutorials, some users may wish to download the overheads used in the videos. Download Chapter Five overheads as a .pdf file. 5.1 Introduction In this section, we learn how to: Record aggregated losses from an insurance system Identify actuarial applications of aggregate loss models Video: Introduction to Aggregate Loss Models Overheads: Introduction to Aggregate Loss Models (Click Tab to View) A. Basic Terminology B. Goal C. Models D. Example E. Applications Hide Hide Hide Hide Hide 5.1.1 Exercise. Claim Frequency Next include one or two exercises per section. Here is an example. Assignment Text The Wisconsin Property Fund data has already been read into a data frame called Insample. These data consist of claim experience for fund members over the years 2006-2010, inclusive. It includes the frequency of claims Freq as well as the claim year Year. The video explored the distribution of the claims frequency for year 2010; in this assignment, we replicate this analysis and conduct a similar investigation for year 2009. Instructions. For each year: Use the function subset() to create a smaller data set based on a single year. Define the frequency as a global variable. Use the function length() to determine the number of observations in a vector. Use the function mean() to calculate the average. Use the function table() to tabulate the frequency distribution. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6Ikluc2FtcGxlIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL09wZW5BY3RUZXh0RGV2L0xEQUNvdXJzZTEvbWFpbi9EYXRhL0luc2FtcGxlLmNzdlwiLCBoZWFkZXI9VCxcbiAgICAgICAgICAgICAgICAgICAgICBuYS5zdHJpbmdzPWMoXCIuXCIpLCBzdHJpbmdzQXNGYWN0b3JzPUZBTFNFKSIsInNhbXBsZSI6IiMgIEFuYWx5c2lzIGZvciBZZWFyIDIwMTBcbkluc2FtcGxlMjAxMCA8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMTApXG5GcmVxMjAxMCA8LSBJbnNhbXBsZTIwMTAkRnJlcVxubGVuZ3RoKEZyZXEyMDEwKVxubWVhbihGcmVxMjAxMClcbnRhYmxlKEZyZXEyMDEwKVxuXG4jICBBbmFseXNpcyBmb3IgWWVhciAyMDA5XG5JbnNhbXBsZTIwMDkgPC0gX19fXG5GcmVxMjAwOSA8LSBfX19cbmxlbmd0aChfX18pXG5tZWFuKF9fXylcbnRhYmxlKF9fXykiLCJzb2x1dGlvbiI6IiMgIEFuYWx5c2lzIGZvciBZZWFyIDIwMTBcbkluc2FtcGxlMjAxMCA8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMTApXG5GcmVxMjAxMCA8LSBJbnNhbXBsZTIwMTAkRnJlcVxubGVuZ3RoKEZyZXEyMDEwKVxubWVhbihGcmVxMjAxMClcbnRhYmxlKEZyZXEyMDEwKVxuXG4jICBBbmFseXNpcyBmb3IgWWVhciAyMDA5XG5JbnNhbXBsZTIwMDkgPC0gc3Vic2V0KEluc2FtcGxlLCBZZWFyPT0yMDA5KVxuRnJlcTIwMDkgPC0gSW5zYW1wbGUyMDA5JEZyZXFcbmxlbmd0aChGcmVxMjAwOSlcbm1lYW4oRnJlcTIwMDkpXG50YWJsZShGcmVxMjAwOSkiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkdldHRpbmcgc3RhcnRlZCBpcyBhbHdheXMgdGhlIGhhcmRlc3QgdGhpbmcgdG8gZG8uIEV4Y2VsbGVudCBzdGFydCFcIikiLCJoaW50IjoiVGFrZSBzb21lIHRpbWUgdG8gZXhwbG9yZSB0aGUgb25saW5lIDxjb2RlPlI8L2NvZGU+IGRvY3VtZW50YXRpb24uPC9wPlxuXG48cD5Ob3RlIHRoZSBkb3VibGUgZXF1YWwgc2lnbnMgPGNvZGU+PT08L2NvZGU+IG5lZWRlZCBmb3IgdGhlIHRoZSA8Y29kZT5zdWJzZXQoKTwvY29kZT4gZnVuY3Rpb24uIn0= Show Quiz Solution 5.2 Individual Risk Model In this section, we learn how to: Build an individual risk model for a portfolio of insurance contracts Apply individual risk model to life and nonlife insurance Compute the distribution of aggregate losses from an individual risk model Video: Individual Risk Model Overheads: Individual Risk Model (Click Tab to View) A. Individual Risk Model B. Applications C. Example D. Aggregate Loss Distribution E. R Example #1 F. R Example #2 Hide Hide Hide Hide Hide Hide Show Quiz Solution 5.3 Collective Risk Model - Part I Start with section learning objectives, often taken directly from the book In this section, we learn how to: Build a collective risk model for a portfolio of insurance contracts Calculate mean and variance of the aggregate loss Fit frequency and severity components in a collective risk model Video: Collective Risk Model - Part I Overheads: Collective Risk Model - Part I (Click Tab to View) A. Collective Risk Model B. Compound Distribution C. Moments D. Model Fitting E. Model Fitting - Property Fund Hide Hide Hide Hide Hide 5.4 Collective Risk Model - Part II In this section, we learn how to: Compute the aggregate loss distribution Implement numerical strategies in R Video: Collective Risk Model - Part II Overheads: Collective Risk Model - Part II (Click Tab to View) A. Computing the Aggregate Loss Distribution B. Direct Calculation C. Direct Calculation - R Code D. Recursive Method E. Recursive Method - R Code F. Monte Carlo Simulation Hide Hide Hide Hide Hide Hide Show Quiz Solution 5.5 Tweedie Distribution In this section, we learn how to: Construct the Tweedie distribution from a collective risk model Establish the Tweedie distribution as a member of the exponential family of distributions Fit Tweedie distribution as a generalized linear model Video: Tweedie Distribution Overheads: Tweedie Distribution (Click Tab to View) A. Tweedie Distribution B. Tweedie Distribution with Exposure C. Distribution Function for Tweedie Distribution D. Example E. Example - R Code Hide Hide Hide Hide Hide 5.6 Effects of Coverage Modifications In this section, we learn how to: Examine the impact of aggregate deductible on the aggregate loss Examine the effect of per-occurrence deductible on frequency and severity components in the aggregate loss Video: Effects of Coverage Modifications Overheads: Effects of Coverage Modifications (Click Tab to View) A. Aggregate Deductible B. Per-occurrence Deductible C. Per-occurrence Deductible and Frequency D. Example E. Per-occurrence Deductible Hide Hide Hide Hide Hide Show Quiz Solution "],
["chapter-template-insert-title-here.html", "Chapter 6 Chapter Template - Insert Title Here 6.1 Section Template - Insert Title Here", " Chapter 6 Chapter Template - Insert Title Here This template illustrates how the pieces of a short course chapter fit together in R markdown. It is not stand-alone; it requires .css and javascript files to support it. You can run this within the R bookdown course that we have developed; simply call it from the bookdown.yml file. Chapter description Start with a chapter intro, often taken directly from the book Also nice to include a downloadable link to chapter overheads. Although not needed to go through the tutorials, some users may wish to download the overheads used in the videos. Download Chapter Two overheads as a .pdf file. 6.1 Section Template - Insert Title Here Start with section learning objectives, often taken directly from the book In this section, we use zzz. You learn how to: zz zz zz. Video: Template - Insert Title Here Insert video location here. Right now hosted by UW-Madison Overheads: Template - Insert Title Here (Click Tab to View) Here is an example of coding the overheads A. Wisconsin Property Fund B. LGPIF Policyholder A C. LGPIF Policyholder B D. Property Fund E. Claims Frequency - R Code F. Claims Frequency (2010) Hide Hide Hide Hide Hide Hide 6.1.1 Exercise. Claim Frequency Next include one or two exercises per section. Here is an example. Assignment Text The Wisconsin Property Fund data has already been read into a data frame called Insample. These data consist of claim experience for fund members over the years 2006-2010, inclusive. It includes the frequency of claims Freq as well as the claim year Year. The video explored the distribution of the claims frequency for year 2010; in this assignment, we replicate this analysis and conduct a similar investigation for year 2009. Instructions. For each year: Use the function subset() to create a smaller data set based on a single year. Define the frequency as a global variable. Use the function length() to determine the number of observations in a vector. Use the function mean() to calculate the average. Use the function table() to tabulate the frequency distribution. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6Ikluc2FtcGxlIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL09wZW5BY3RUZXh0RGV2L0xEQUNvdXJzZTEvbWFpbi9EYXRhL0luc2FtcGxlLmNzdlwiLCBoZWFkZXI9VCxcbiAgICAgICAgICAgICAgICAgICAgICBuYS5zdHJpbmdzPWMoXCIuXCIpLCBzdHJpbmdzQXNGYWN0b3JzPUZBTFNFKSIsInNhbXBsZSI6IiMgIEFuYWx5c2lzIGZvciBZZWFyIDIwMTBcbkluc2FtcGxlMjAxMCA8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMTApXG5GcmVxMjAxMCA8LSBJbnNhbXBsZTIwMTAkRnJlcVxubGVuZ3RoKEZyZXEyMDEwKVxubWVhbihGcmVxMjAxMClcbnRhYmxlKEZyZXEyMDEwKVxuXG4jICBBbmFseXNpcyBmb3IgWWVhciAyMDA5XG5JbnNhbXBsZTIwMDkgPC0gX19fXG5GcmVxMjAwOSA8LSBfX19cbmxlbmd0aChfX18pXG5tZWFuKF9fXylcbnRhYmxlKF9fXykiLCJzb2x1dGlvbiI6IiMgIEFuYWx5c2lzIGZvciBZZWFyIDIwMTBcbkluc2FtcGxlMjAxMCA8LSBzdWJzZXQoSW5zYW1wbGUsIFllYXI9PTIwMTApXG5GcmVxMjAxMCA8LSBJbnNhbXBsZTIwMTAkRnJlcVxubGVuZ3RoKEZyZXEyMDEwKVxubWVhbihGcmVxMjAxMClcbnRhYmxlKEZyZXEyMDEwKVxuXG4jICBBbmFseXNpcyBmb3IgWWVhciAyMDA5XG5JbnNhbXBsZTIwMDkgPC0gc3Vic2V0KEluc2FtcGxlLCBZZWFyPT0yMDA5KVxuRnJlcTIwMDkgPC0gSW5zYW1wbGUyMDA5JEZyZXFcbmxlbmd0aChGcmVxMjAwOSlcbm1lYW4oRnJlcTIwMDkpXG50YWJsZShGcmVxMjAwOSkiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkdldHRpbmcgc3RhcnRlZCBpcyBhbHdheXMgdGhlIGhhcmRlc3QgdGhpbmcgdG8gZG8uIEV4Y2VsbGVudCBzdGFydCFcIikiLCJoaW50IjoiVGFrZSBzb21lIHRpbWUgdG8gZXhwbG9yZSB0aGUgb25saW5lIDxjb2RlPlI8L2NvZGU+IGRvY3VtZW50YXRpb24uPC9wPlxuXG48cD5Ob3RlIHRoZSBkb3VibGUgZXF1YWwgc2lnbnMgPGNvZGU+PT08L2NvZGU+IG5lZWRlZCBmb3IgdGhlIHRoZSA8Y29kZT5zdWJzZXQoKTwvY29kZT4gZnVuY3Rpb24uIn0= Finish the section with quizzes from the book. We can modify them, if needed. Show Quiz Solution "]
]
