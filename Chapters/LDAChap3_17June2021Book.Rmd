
# Modeling Loss Severity

**Draft - Chapter Not Yet Complete**

<br>

**Chapter description.** The traditional loss distribution approach to modeling aggregate losses starts by separately fitting a frequency distribution to the number of losses and a severity distribution to the size of losses. The estimated aggregate loss distribution combines the loss frequency distribution and the loss severity distribution by convolution. Discrete distributions often referred to as counting or frequency distributions were used in Chapter 2 to describe the number of events such as number of accidents to the driver or number of claims to the insurer. Lifetimes, asset values, losses and claim sizes are usually modeled as continuous random variables and as such are modeled using continuous distributions, often referred to as loss or severity distributions. A mixture distribution is a weighted combination of simpler distributions that is used to model phenomenon investigated in a heterogeneous population, such as modeling more than one type of claims in liability insurance (small frequent claims and large relatively rare claims). In this chapter we explore the use of continuous as well as mixture distributions to model the random size of loss. 


:::: {.blackbox }

-  Although not needed to go through the tutorials, some users may wish to download the overheads used in the videos. <button download><a href="https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/LDA1.Overheads/LDA1.Chap3.pdf">Download Chapter Three overheads as a .pdf file.</a></button>
-  By watching the videos and working through the tutorial exercises, you will get an appreciation for distributions used to model the severity of losses. For a deeper dive, see the corresponding chapter in the textbook, [Chapter Three of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Severity.html).

:::: 

## Basic Distributional Quantites

***

In this section, you learn how to work with some basic distributional quantities:

*    moments,
*    percentiles, and
*    generating functions.

***

### Exercise. Moments Can Be Misleading

**Assignment Text**

Anscombe's quartet is comprised of four datasets with interesting properties related to their moments. You can access the data via `data(anscombe)`. The four data sets are in the variables `y1`, `y2`, `y3` and `y4`. There are also associated `x` values that create interesting properties for regression models fit to the data. We'll ignore the `x` values in this section but will use them to develop the plots below.

```{r anscombe plot, echo=FALSE}
data("anscombe")
op <- par(mfrow = c(2, 2), mar = 0.1 + c(4,4,1,1), oma =  c(0, 0, 2, 0))
invisible(lapply(X = 1:4, FUN = function(no){
  plot(x = anscombe[[paste0('x', no)]], y = anscombe[[paste0('y', no)]],
    type = 'p', xlab = 'x', ylab = 'y')
}))
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
```

:::: {.blackbox }

**Instructions**

*  Extract the values `y1`, `y2`, `y3` and `y4` from the `anscombe` `data frame`. You will want to extract these as vectors not as single-column data frames.  
*  Use [`summary`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) to understand the basic distributional properties of `y1`, `y2`, `y3` and `y4`.
*  Use [`mean`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean) to calculate the first raw moment of `y1`, `y2`, `y3` and `y4`. Assign the values to `y_means`.
*  Use [`var`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cor) to calculate the second raw moment of `y1`, `y2`, `y3` and `y4`. You should note that the the `var` function uses $n-1$ in the denominator to return an estimate of the population variance from the sample. Do you notice anything interesting about the results?
*  Use [`sd`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sd) to calculate the standard deviation of `y1`, `y2`, `y3` and `y4`. As with the `var` function`,`sd` also uses $n-1$ in the denominator.
*  There is no `base-R` function to calculate the coefficient of skewness. There are several approaches to calculate skewness if you explore the [help](https://www.rdocumentation.org/packages/e1071/versions/1.7-4/topics/skewness) for the `type` argument of the `skewness` function from the `e1071` package.  We will note use this package, but instead we will we calculate the equivalent of `type = 3` skewness using `var` in the denominator.

::::

<br>

```{r ex="LDA1.3.1.1", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation.
```

```{r ex="LDA1.3.1.1", type="pre-exercise-code", tut=TRUE}
data("anscombe")
install.packages('moments')
library(moments)

```

```{r ex="LDA1.3.1.1", type="sample-code", tut=TRUE}
#Summary of Data
summary(anscombe$y1) #Example for the first data set in  Anscombe's quartet
summary(anscombe$??)
summary(anscombe$??)
summary(anscombe$??)

#Vector of means
c(mean(anscombe$y1), mean(??), mean(??), mean(??))

#Vector of variances
c(var(anscombe$y1), var(??), var(??), var(??))

#Skewbess if y1
y1_skew <- mean((?? - mean(??))^??) / sd(??) ^ ??

```


```{r ex="LDA1.3.1.1", type="solution", tut=TRUE}
#Summary of Data
summary(anscombe$y1) #Example for the first data set in  Anscombe's quartet
summary(anscombe$y2)
summary(anscombe$y3)
summary(anscombe$y4)

#Vector of means
c(mean(anscombe$y1), mean(anscombe$y2), mean(anscombe$y3),
  mean(anscombe$y4))

#Vector of variances
c(var(anscombe$y1), var(anscombe$y2), var(anscombe$y3),
  var(anscombe$y4))
y1_skew <- mean((anscombe$y1 - mean(anscombe$y1))^3) / sd(anscombe$y1) ^ 3

```


```{r ex="LDA1.3.1.1", type="sct", tut=TRUE}
success_msg("Well done!")
```
### Exercise. Determining Boxplots and Quantiles

**Assignment Text**

The `boxplot` function produces box-and-whisker plots of grouped values. The function returns certain statistic and produces a plot as a side effect. Explore the  [`boxplot`](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/boxplot) to understand the values returned. In particular review the the information returned by the `stats` object.

The Wisconsin Property Fund data has already been read into a data frame called `Insample`. These data consist of claim experience for fund members over the years 2006-2010, inclusive. It includes claim values `y` as well as the claim year `Year`. For this exercise we will work with the natural logarithm of the claim values which are in the `lny` variable. We have filtered the data to exclude zero claims and saved the resulting data frame as `Insample_nz`.

:::: {.blackbox }

**Instructions**

We provide the code to generate the boxplot. Note that the `R` creates the boxplot as a side effect while returning information about the data. (`R` does not print the information to the console by deafult. You'll have to assign the return of the boxplot function to an object to access.)

* What is the class of the of the object returned by the boxplot function.    
* Access the `stats` object returned.    
* Use the `quantile` function and replicate the "hinge" values for 2008 box. We will ignore the values returned for the whiskers as they are not a function of the quantiles of the distribution.    
*  We selected the 2008 year because it has an odd number of observations. Use the `quantile` function on a vector with an even number such as `1:100` and access the 25th percentile. Are you surprised by the result? Review the details for the `type` argument to the [`quantile`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile) to understand the various approached to determining `quantiles`.  

::::

<br>

```{r ex="LDA1.3.1.2", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation.
```


```{r ex="LDA1.3.1.2", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors = FALSE)
Insample_nz <- Insample[Insample$y > 0,]
```

```{r ex="LDA1.3.1.2", type="sample-code", tut=TRUE}
my_boxplot <- boxplot(formula = ??, data = Insample_nz)
class(??)
my_boxplot$??
my_boxplot$stats[,_] #2008

lny_2008 <- Insample_nz$lny[Insample_nz$Year == 2008]

quantile(lny_2008, ??) # lower hinge
??(lny_2008) # median through a dedicated function
quantile(lny_2008, ??) # median
quantile(lny_2008, ??) # upper hinge

```


```{r ex="LDA1.3.1.2", type="solution", tut=TRUE}
my_boxplot <- boxplot(formula = lny ~ Year, data = Insample_nz)
class(my_boxplot)
my_boxplot$stats
my_boxplot$stats[,3] #2010
lny_2008 <- Insample_nz$lny[Insample_nz$Year == 2008]

quantile(lny_2008, 1/4) # lower hinge
median(lny_2008) # median
quantile(lny_2008, 3/4) # upper hinge

```


```{r ex="LDA1.3.1.2", type="sct", tut=TRUE}
success_msg("Well done!")
```

### Exercise. Gamma Moment Generating Function

**Assignment Text**

In Example 3.1.4 of the text, we presented the following moment generating function for a gamma distribution,

$$
M_X(t) = (1 - \theta t)^{-\alpha} .
$$

We will work with this distribution in these exercises. The parameters that we will use in our exercise are $\alpha=2$ (the shape parameter) and $\theta = 10$ (the scale parameter). Keep in mind that the mean for this distribution is $\alpha\theta=20$ and the variance is $\alpha\theta^2 = 200$. We will revisit these results at the end of the excercise.

There are several `R` functions that you will find useful:

-   The [`parse`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/parse) function converts a character vector to an [`expression`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/expression). Then, you can use the [`eval`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/eval) to evaluate the expression.
-   You can use the [`D`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/deriv) to compute derivatives.

:::: {.blackbox }

**Instructions**

-   Use the `parse` function to create an expression and for the moment generating function. Specify the expression using `alpha`, `theta` and `t`. Name the object `mgf`.
-   Note the `class` of `mgf`.
-   Evaluate `mgf` at `t = 0` when `alpha = 2` and `theta = 10`. (Note that you do not need to specify the values of `alpha` and `theta` and `t` in the `eval` call. That is because we did not specify the `envir` argument to `eval` and the default is `parent.frame()`. Environments are a more advanced `R` topic that you can learn about [here](https://adv-r.hadley.nz/environments.html).

:::: 

<br>

```{r ex="LDA1.3.1.3.1", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation. In particular, take time to understand the required class of each argument.
```

```{r ex="LDA1.3.1.3.1", type="pre-exercise-code", tut=TRUE}

```

```{r ex="LDA1.3.1.3.1", type="sample-code", tut=TRUE}

mgf <- parse(text = "??")

class(x = ??)

t <- ??
alpha <- ??
theta <- ??

eval(expr = ??)

```

```{r ex="LDA1.3.1.3.1", type="solution", tut=TRUE}
mgf <- parse(text = "(1- theta * t) ^ -alpha")

class(x = mgf)

alpha <- 2
theta <- 10
t <- 0

eval(expr = mgf)
```

```{r ex="LDA1.3.1.3.1", type="sct", tut=TRUE}
success_msg("Well done!")
```

### Exercise. Visualizing Moment Generating Functions

**Assignment Text**

We plot *mgf* to help visualize the function. The left-hand panel shows the function with the argument $t$ is between -0.2 and 0.2. 
For $\theta = 10$ and $\alpha > 0$, note that *mgf* goes to `+inf` when $t = \frac{1}{\theta} = 0.1$. The right-hand panel shows the function between -0.01 and 0.01. This provides a better view of the curvature at zero.

```{r echo=FALSE, out.width='80%', fig.asp=.60}
mgf_fn <- function(x, alpha = 2, theta = 10) (1 - theta * x) ^ (-alpha)
par(mfrow=c(1,2))
curve(mgf_fn, ylab= 'mgf(t)', xlab = 't', from=-.2, to=.2, ylim = c(0,100))
abline(v=0.1, col = "green", lty = 3)
curve(mgf_fn, ylab = 'mgf(t)', xlab = 't', from=-0.01, to=0.01)
abline(v=0, col = "blue", lty = 3)
```


```{r eval = FALSE, echo=FALSE, out.width='80%', fig.asp=.80}
tbase <- seq( from=0, to=2, length.out =1000)
mgf_fn <- function(x, alpha = 2, theta = 10) (1 - theta * x) ^ (-alpha)
fun.val <- mgf_fn(tbase)
plot(tbase, fun.val, ylim = c(0,120), type = "l")

```

**Instruction**

-   Use the `D` function to return an expression for the first and second derivatives with respect to $t$. Assign the results of `dMxtdt` and `d2Mxtdt2`, respectively.

```{r ex="LDA1.3.1.3.2", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation. In particular, take time to understand the required class of each argument.
```


```{r ex="LDA1.3.1.3.2", type="sample-code", tut=TRUE}

dMxtdt <- D(expr = parse(text = "??"), name = '??')

d2Mxtdt2 <- D(
  D(expr = parse(text = "??"), name = '_'),
  "t")


```

```{r ex="LDA1.3.1.3.2", type="solution", tut=TRUE}
Mxtdt <- D(expr = parse(text = "(1- theta * t) ^ -alpha"), name = 't')

Mxtdt

M2xtdt <- D(
  D(expr = parse(text = "(1- theta * t) ^ -alpha"), name = 't'),
  name = 't')

M2xtdt

```

```{r ex="LDA1.3.1.3.2", type="sct", tut=TRUE}
success_msg("Well done!")
```

### Exercise. Determining Moments using Moment Generating Functions

**Assignment Text**

Recall that you can determine the $n^{th}$ moment of of $X$ as the $n^{th}$ derivative of the moment generating function evaluated at $t = 0$.

:::: {.blackbox }

**Instructions**

-   Calculate the mean (first moment) of the gamma distribution.
-   Calculate the second raw moment of the gamma distribution.
-   Calculate the variance (second central moment) of the gamma distribution

::::

<br>

```{r ex="LDA1.3.1.3.3", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation. In particular, take time to understand the required class of each argument.
```


```{r ex="LDA1.3.1.3.3", type="sample-code", tut=TRUE}

# mean 
EX <- eval(??)

# second raw moment
EX2 <- eval(??)

# variance

?? - ?? ^ 2

```

```{r ex="LDA1.3.1.3.3", type="solution", tut=TRUE}

# mean 
EX <- eval(Mxtdt)

# second raw moment
EX2 <- eval(M2xtdt)

# variance
EX2 - EX ^ 2

```

```{r ex="LDA1.3.1.3.3", type="sct", tut=TRUE}
success_msg("Well done!")
```


## Continuous Distributions

***

In this section, you learn how to define and apply four fundamental severity distributions:

-   gamma,
-   Pareto,
-   Weibull, and
-   generalized beta distribution of the second kind.

`R` provides a "family" of functions associated with statistical distributions. The naming convention is to precede the distribution name with `d` for the density function, `p` for the cumulative distribution function, `q` for quantiles (percentiles), and `r` for random number generation. (We will set aside the `r` functions for Chapter 6 on Simulation and focus on `d`, `p`, and `q` functions.)

The `stats` packages which is provided with base-`R` includes the distributions listed [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Distributions). Many other distributions are available in other `R` packages.

For the exercises in the section, we will focus on the gamma distribution from [Section 3.2.1](https://openacttexts.github.io/Loss-Data-Analytics/C-Severity.html#S:Loss:Gamma) to help you develop an intuition as to how the distribution responds to changes in its shape and scale parameter. From Figure 3.1, you should recognize that the support for the gamma distribution is positive real numbers. You'll also note that the distribution allows for varying levels of "right-skewness."

***


### Exercise. 

**Assignment Text**

We will be working with the Wisconsin Property Fund data. We have read the data and created a vector of the `log` of non-zero claim values. The name of that vector is `wisc_prop.`

:::: {.blackbox }

**Instructions**

-   Use the `hist` function to plot a histogram of `wisc_prop`.
-   Review the help for [`hist`](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/hist) and note that the histogram is drawn as a "side-effect." We can access the information generated with the histogram by assigning the result to an object. For this exercise, you should assign that information to `hist_data`. Note the class of `hist_data`.
-   Use the `str` function to review the elements of the list returned by the `histogram` function.

:::: {.blackbox }

<br>

```{r ex="LDA1.3.2.1", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation. In particular, take time to understand the required class of each argument.
```

```{r ex="LDA1.3.2.1", type="pre-exercise-code", tut=TRUE}
wisc_prop <- read.csv("https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors = FALSE)
wisc_prop <- log(wisc_prop$y[wisc_prop$y > 0])

```

```{r ex="LDA1.3.2.1", type="sample-code", tut=TRUE}

hist_data <- hist(x = "??")

class(x = ??)

str(object = ??)
```

```{r ex="LDA1.3.2.1", type="solution", tut=TRUE}
hist_data <- hist(wisc_prop)
class(hist_data)
str(hist_data)
```

```{r ex="LDA1.3.2.1", type="sct", tut=TRUE}
success_msg("Well done!")
```

### Assignment Text

From the histogram, we will develop an intuition as to whether a gamma distribution is likely to fit this data reasonably well. Consider whether the data is uni-modal and whether it is right-skewed. It appears to be which would support the modeling of log-claim sizes using the gamma distribution.

You'll note that the $y$-axis of the histogram displays counts. In contrast, the `dgamma` function returns density values. So we can't add a gamma distribution to the histogram and compare to the data. Instead we first need to plot the density values of the data. To plot probability densities, we will use set the `probability` argument to hist to `TRUE`. You should note the following from the documentation of `hist`:

-   The relationship between the that the `probability` and `freq` arguments to `hist`.
-   There is a `density` argument to `hist`. As first, you may think this refers to the (probability) density function, but that argument specifies a plotting feature.

You can use the [`curve`](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/curve) function to plot the gamma distribution over the histogram data. To do this, you need to set the `add` argument to `TRUE`.

In later sections, we'll review approaches to parameter estimation. In this section, we will vary the `shape` and `scale` parameters to understand the flexibility of the gamma distribution. The data will serve as a reference for this exercise. You may recall from the exercises from Section 3.1.3 that the mean of the gamma function is the product of the `shape` and `scale` parameters and the variance is the product of the `shape` and square of the `scale` parameters. You can use this information to develop parameter estimates using the method of moments.

:::: {.blackbox }

**Instructions**

-   Again plot the histogram. This time specify the probability argument so as to plot probability densities.
-   Calculate `shape` and `scale` parameters using the method of moments
-   Calculate the density values for $x$ values displayed within the limits of the plot.
-   Add `3` to the `shape` parameter and note how the distribution changes.
-   Subtract `0.05` from the `scale` parameter and note how the distribution changes.

:::: 

<br>

```{r ex="LDA1.3.2.2", type="pre-exercise-code", tut=TRUE}
wisc_prop <- read.csv("https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors = FALSE)
wisc_prop <- log(wisc_prop$y[wisc_prop$y > 0])
```

```{r ex="LDA1.3.2.2", type="sample-code", tut=TRUE}

hist_data <- hist(x = ??, probability = ??)
gamma_scale <- var(??) / mean(??)
gamma_shape <- mean(??) / gamma_scale
curve(dgamma(x, shape = ??, scale = ??), add = TRUE, col = 'red' )
gamma_scale
gamma_shape
curve(dgamma(x, shape = ?? + ??, scale = ??), add = TRUE, col = 'blue')
curve(dgamma(x, shape = ??, scale = ?? -  ??), add = TRUE, col = 'green')
```


```{r ex="LDA1.3.2.2", type="solution", tut=TRUE}
hist_data <- hist(x = wisc_prop, probability = TRUE)
gamma_scale <- var(wisc_prop) / mean(wisc_prop)
gamma_shape <- mean(wisc_prop) / gamma_scale
curve(dgamma(x, shape = gamma_shape, scale = gamma_scale), add = TRUE, col = 'red' )
gamma_scale
gamma_shape
curve(dgamma(x, shape = gamma_shape + 3, scale = gamma_scale +  0), add = TRUE, col = 'blue')
curve(dgamma(x, shape = gamma_shape + 0, scale = gamma_scale -  0.05), add = TRUE, col = 'green')
```

```{r ex="LDA1.3.2.2", type="sct", tut=TRUE}
success_msg("Well done!")
```


## Methods of Creating New Distributions

***

In this section, you learn how to:

-  Understand connections among the distributions
-  Give insights into when a distribution is preferred when compared to alternatives
-  Provide foundations for creating new distributions

***


####  Video: Creating Distributions I {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_3ossuexn&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_x80k8dtk" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>


####  Video: Creating Distributions II {-}


<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_5023lgkw&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_m33sbiz0" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>




## Coverage Modifications

***

In this section we evaluate the impacts of coverage modifications: 

-  deductibles, 
-  policy limit, 
-  coinsurance, and 
-  reinsurance on insurerâ€™s costs.

***

####  Video: Policy Deductibles {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_v4oyj7ne&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_mnc9h2ja" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>


####  Video: Policy Limits {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_7u0jx487&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_gol12ato" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>



**Assignment Text**

The support of the lognormal distribution is positive real numbers and the distribution is right-skewed. Given these properties, claim values for many coverages follow a lognormal distribution.  

`R` provides many several functions related to the [lognormal](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Lognormal).

Particularly useful for calculations related to the effect of coverage modifications, the [`plnorm`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Lognormal) function returns the distribution function for a lognormal. Specifically, the limited expected value function is useful for calculating limited claim values, that is, claims amounts subject to policy limits. 

The limited expected value of $X$ at limit, $L$, is calculated as follows:

$$
LEV[x;L] = \int_0^L x f_X(x) dx ~+ L * [1 - F_X(L)] .
$$

The first term on the right-hand side is the contribution to the expected value for claims that are less than $L$ and the second term is the expected value of all claims greater than or equal to $L$. 

For the exercises in this section, we provide the `mlnorm` and `levlnorm` function to calculate unlimited mean and limited expected values, respectively with arguments `meanlog` and `sdlog` corresponding to the same arguments in the `plnorm` function.  The `levlnorm` function requires an additional `limit` argument.

We also provide the vector `claims` which are observed claims values from lognormal distribution with `meanlog` = 12 and `sdlog` = 1.5. The values of `meanlog` and `sdlog` are saved in the `mu` and `sigma` objects.

:::: {.blackbox }

**Instructions**

* Calculate the expected value of the lognormal function and compare to the `mean` of `claims`.
* Calculate the expected value of the claims retained by the insured using the `levlnorm` function and the compare to the mean of `claims`.

:::: 

<br>

```{r ex="LDA1.3.4.1", type="hint", tut=TRUE}
Note that we have created the `R` objects `mu` and `sigma`, consider how to use them in your answer.
Some coders find it easier to code thousands and millions using scientific notation. For example, it may be easier to type 5e6 than to count the number of zeros on in 5000000 on your screen.
```


```{r ex="LDA1.3.4.1", type="pre-exercise-code", tut=TRUE}
set.seed(12345)
mu <- 12
sigma <- 1.5
claims <- rlnorm(n = 100000, meanlog = mu, sdlog = sigma)
mlnorm <- function(meanlog, sdlog) exp(meanlog + sdlog ^ 2 /2)

levlnorm <- function(limit, meanlog, sdlog){
  t1 <- mlnorm(meanlog = meanlog, sdlog = sdlog) * 
    pnorm(q = (log(limit) - meanlog - sdlog ^ 2) / sdlog)
  t2 <- limit * ( 1- pnorm((log(limit) - meanlog) / sdlog))
  return(t1 + t2)
  }
```

```{r ex="LDA1.3.4.1", type="sample-code", tut=TRUE}

# Unlimited
#expected value of the lognormal function
mlnorm(meanlog = ??, sdlog = ??)
# mean of claims
mean(??)

```

```{r ex="LDA1.3.4.1", type="solution", tut=TRUE}

# Unlimited
#expected value of the lognormal function
mlnorm(meanlog = mu, sdlog = sigma)
# mean of claims
mean(claims)
```

```{r ex="LDA1.3.4.1", type="sct", tut=TRUE}
success_msg("Well done!")
```

**Assignment Text**

Now, assume that an insurer provides coverage for the first $5 million of claims.

:::: {.blackbox }

**Instructions**

* Calculate the expected value of the claims insured with a $5 million policy limit using the `levlnorm` function and the compare to the amounts insured from `claims`.

::::

<br>

```{r ex="LDA1.3.4.2", type="sample-code", tut=TRUE}

### Claim amounts insured under a $5 million limit    

# From the limited expected value function
levlnorm(limit = ??, meanlog = ??, sdlog = ??)

# From the claims data
mean(pmin(??, ??))
```

```{r ex="LDA1.3.4.2", type="solution", tut=TRUE}

### Claim amounts insured under a $5 million limit    

# From the limited expected value function
levlnorm(limit = 5e6, meanlog = 12, sdlog = 1.5)


# From the claims data
mean(pmin(5e6, claims))
```


```{r ex="LDA1.3.4.2", type="sct", tut=TRUE}
success_msg("Well done!")
```


**Assignment Text**

When an insurer provides coverage with a deductible, only a portion of claims result in reimbursement. The claims that do produce reimbursement have a greater average value than the average value for all claims.

For this exercise, assume that an insurer provides coverage for the first \$5 million of claims in excess of a \$1 million deductible.  Note that this coverage is sometimes states as \$4 million excess of \$1 million (or shorthand notation such as \$4mln xs \$1 mln).

:::: {.blackbox }

**Instructions**

*  Determine the number of `claims` resulting in reimbursement. Compare to the expected value of the number of claims requiring reimbursement calculated using the `plnorm` function.
*  Calculate the amounts insured. Do this in two steps. First, subset `claims` to only inlcue amounts in excess of $1 million. Second, calculate the average value of those claims.
*  Calculate the expected value of the claims insured subject to a \$1 million deductible and a $5 million policy limit using the `levlnorm` and `plnorm` function and the compare to the amounts you calculated in the prior step. Note that these are the 'ground-up' values meaning that they include the amounts within the deductible.

:::: 

<br>

```{r ex="LDA1.3.4.3", type="sample-code", tut=TRUE}

### Claim counts amounts insured under a $1 million deductible applied to the first $5 million of claims
# Number of claims in the data
sum(claims > ??) 
# Expected
(1 - plnorm(q = ??, meanlog = ??, sdlog = ??)) * length(??)

# Average Value from claims
insured <- claims[claims > ??]
mean(pmin(??, ??))

#Expected
?? + (levlnorm(limit = ??, meanlog = ??, sdlog = ??) - 
  levlnorm(limit = ??, meanlog = ??, sdlog = ??)) / 
  (1 - plnorm(q = ??, meanlog = ??, sdlog = ??))

```

```{r ex="LDA1.3.4.3", type="solution", tut=TRUE}
### Claim counts amounts insured under a $1 million deductible applied to the first $5 million of claims
# Number of claims in the data
sum(claims > 1e6) 
# Expected
(1 - plnorm(q = 1e6, meanlog = mu, sdlog = sigma)) * length(claims)

# Average Value from claims
insured <- claims[claims > 1e6]
mean(pmin(5e6, insured))

#Expected
1e6 + (levlnorm(limit = 5e6, meanlog = mu, sdlog = sigma) - 
  levlnorm(limit = 1e6, meanlog = mu, sdlog = sigma)) / 
  (1 - plnorm(q = 1e6, meanlog = mu, sdlog = sigma))
```

```{r ex="LDA1.3.4.3", type="sct", tut=TRUE}
success_msg("Well done!")
```

**Assignment Text**

For this exercise, assume that an insurer provides coverage for the first \$5 million of claims in excess of a \$1 million deductible.  Note that this coverage is sometimes states as \$4 million excess of \$1 million (or shorthand notation such as \$4mln xs \$1 mln). (This is the same as the prior exercise.) However, now you should assume that there is a 20% coinsurance provision. The number of claims will not differ from the prior exercise, so will focus on the average value.

:::: {.blackbox }

**Instructions**

* Calculate the expected insured value of the claims subject to a \$1 million deductible and 20% coinsurance and a $5 million policy limit using the `levlnorm` and `plnorm` function and the compare to the amounts insured from `claims`. In contrast to the prior excercise, calculate the insured value of the claim rather than the ground-up value.

:::: 

<br>

```{r ex="LDA1.3.4.4", type="sample-code", tut=TRUE}

### Claim amounts insured under a $1 million deductible with a $5 million policy limit
### with 20% coinsurance

#Average Value
insured <- claims[claims > ??]
mean(pmin(??, ??) - ??) * (1 - ??)

#Average Value
(levlnorm(limit = ??, meanlog = ??, sdlog = ??) - 
  levlnorm(limit = ??, meanlog = ??, sdlog = ??)) / 
  (1 - plnorm(q = ??, meanlog = ??, sdlog = ??)) * 
    (1 - ??)
```


```{r ex="LDA1.3.4.4", type="solution", tut=TRUE}

### Claim amounts insured under a $1 million deductible with a $5 million policy limit   
### with 20% coinsurance

#Average Value
insured <- claims[claims > 1e6]
mean(pmin(5e6, insured) - 1e6) * (1 - 0.2)

#Average Value

(levlnorm(limit = 5e6, meanlog = mu, sdlog = sigma) - 
  levlnorm(limit = 1e6, meanlog = mu, sdlog = sigma)) / 
  (1 - plnorm(q = 1e6, meanlog = mu, sdlog = sigma)) * (1 - 0.2)
```


```{r ex="LDA1.3.4.4", type="sct", tut=TRUE}
success_msg("Well done!")
```

####  Video: Policy Coinsurance and Reinsurance {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_4f9j72l3&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5vv01as5" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

## Maximum Likelihood Estimation

***

In this section, you learn how to:

*    Define a likelihood for a sample of observations from a continuous distribution
*    Define the maximum likelihood estimator for a random sample of observations from a continuous distribution
*    Estimate parametric distributions based on grouped, censored, and truncated data

***



